{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleSpread_MADDPG.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xlCDg0JW98O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "58aa20bd-046e-4a5c-8f59-bf69b5b90f36"
      },
      "source": [
        "!git clone https://github.com/openai/multiagent-particle-envs.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'multiagent-particle-envs'...\n",
            "remote: Enumerating objects: 234, done.\u001b[K\n",
            "remote: Total 234 (delta 0), reused 0 (delta 0), pack-reused 234\u001b[K\n",
            "Receiving objects: 100% (234/234), 100.83 KiB | 2.80 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GjEH2r2XHnv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "7628f005-e26a-48be-dde6-f35442d8b470"
      },
      "source": [
        "%cd multiagent-particle-envs\n",
        "!ls\n",
        "!pip install -e ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/multiagent-particle-envs\n",
            "bin  LICENSE.txt  make_env.py  multiagent  README.md  setup.py\n",
            "Obtaining file:///content/multiagent-particle-envs\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from multiagent==0.0.1) (0.17.2)\n",
            "Collecting numpy-stl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/08/2d8533798a08e1878120a1bf4970eb8ee50f6860cd50db917c9defe5dda2/numpy-stl-2.11.2.tar.gz (484kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->multiagent==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->multiagent==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->multiagent==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->multiagent==0.0.1) (1.18.4)\n",
            "Requirement already satisfied: python-utils>=1.6.2 in /usr/local/lib/python3.6/dist-packages (from numpy-stl->multiagent==0.0.1) (2.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->multiagent==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from python-utils>=1.6.2->numpy-stl->multiagent==0.0.1) (1.12.0)\n",
            "Building wheels for collected packages: numpy-stl\n",
            "  Building wheel for numpy-stl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpy-stl: filename=numpy_stl-2.11.2-cp36-cp36m-linux_x86_64.whl size=134859 sha256=cf0e5bbb9af469ebfe8fd3f6527810d2e6f5070bc213534afd65bbdca3a3e0c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/c8/18/436f6b7a2601408d9e5f8c20afb4f5cac5ef0dabe222becbf4\n",
            "Successfully built numpy-stl\n",
            "Installing collected packages: numpy-stl, multiagent\n",
            "  Running setup.py develop for multiagent\n",
            "Successfully installed multiagent numpy-stl-2.11.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKUAKeieYcBf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "c38bbe10-2013-4ba2-891d-c9ca6ac3dd15"
      },
      "source": [
        "!pip install gym==0.10.5"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym==0.10.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/50/ed4a03d2be47ffd043be2ee514f329ce45d98a30fe2d1b9c61dea5a9d861/gym-0.10.5.tar.gz (1.5MB)\n",
            "\r\u001b[K     |▏                               | 10kB 18.4MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██▉                             | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |███                             | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████                            | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▌                           | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 686kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 696kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 706kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 716kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 727kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 737kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 747kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 757kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 768kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 778kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 788kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 798kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 808kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 819kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 829kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 839kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 849kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 860kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 870kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 880kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 890kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 901kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 911kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 921kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 931kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 942kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 952kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 962kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 972kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 983kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 993kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.3MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.3MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.4MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.4MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.4MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.4MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.4MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.5MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.5MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.5MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.5MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.5) (1.18.4)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.5) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.10.5) (1.12.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.5) (1.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.5) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.5) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.5) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym==0.10.5) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.5-cp36-none-any.whl size=1581309 sha256=a82f4a6721beb8294b034e29e157e0297edf91c1e767713071d944c477f4a8cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/14/71/f4ab006b1e6ff75c2b54985c2f98d0644fffe9c1dddc670925\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Found existing installation: gym 0.17.2\n",
            "    Uninstalling gym-0.17.2:\n",
            "      Successfully uninstalled gym-0.17.2\n",
            "Successfully installed gym-0.10.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSNaBJU4v0S",
        "colab_type": "text"
      },
      "source": [
        "# **Model: ACTOR AND CRITIC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcnpEgl-4laR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-ED-hiu5Kdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CentralizedCritic(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_dim, action_dim):\n",
        "        super(CentralizedCritic, self).__init__()\n",
        "\n",
        "        # obs_dim = n_agents * local_obs_dim\n",
        "        self.obs_dim = obs_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        self.linear1 = nn.Linear(self.obs_dim, 1024)\n",
        "        self.linear2 = nn.Linear(1024 + self.action_dim, 512)\n",
        "        self.linear3 = nn.Linear(512, 300)\n",
        "        self.linear4 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        xa_cat = torch.cat([x,a], 1)\n",
        "        xa = F.relu(self.linear2(xa_cat))\n",
        "        xa = F.relu(self.linear3(xa))\n",
        "        qval = self.linear4(xa)\n",
        "\n",
        "        return qval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzqmCvYe8Ptu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_dim, action_dim):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.obs_dim = obs_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        self.linear1 = nn.Linear(self.obs_dim, 512)\n",
        "        self.linear2 = nn.Linear(512, 128)\n",
        "        self.linear3 = nn.Linear(128, self.action_dim)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        x = F.relu(self.linear1(obs))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = torch.tanh(self.linear3(x))\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIymL8FtZiYc",
        "colab_type": "text"
      },
      "source": [
        "# **Agent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiGU6tR9bQbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-ANlVzr9Chl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DDPGAgent:\n",
        "\n",
        "    def __init__(self, env, agent_id, actor_lr=1e-4, critic_lr=1e-3, gamma=0.99, tau=1e-2):\n",
        "        self.env = env\n",
        "        self.agent_id = agent_id\n",
        "        self.actor_lr = actor_lr\n",
        "        self.critic_lr = critic_lr\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "\n",
        "        self.device = \"cpu\"\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        if self.use_cuda:\n",
        "            self.device = \"cuda\"\n",
        "\n",
        "        self.obs_dim = self.env.observation_space[agent_id].shape[0]\n",
        "        self.action_dim = self.env.action_space[agent_id].n\n",
        "        self.num_agents = self.env.n\n",
        "\n",
        "        self.critic_input_dim = int(np.sum([env.observation_space[agent].shape[0] for agent in range(env.n)]))\n",
        "        self.actor_input_dim = self.obs_dim\n",
        "\n",
        "        self.critic = CentralizedCritic(self.critic_input_dim, self.action_dim * self.num_agents).to(self.device)\n",
        "        self.critic_target = CentralizedCritic(self.critic_input_dim, self.action_dim * self.num_agents).to(self.device)\n",
        "        self.actor = Actor(self.actor_input_dim, self.action_dim).to(self.device)\n",
        "        self.actor_target = Actor(self.actor_input_dim, self.action_dim).to(self.device)\n",
        "\n",
        "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "        \n",
        "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "        \n",
        "        self.MSELoss = nn.MSELoss()\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = autograd.Variable(torch.from_numpy(state).float().squeeze(0)).to(self.device)\n",
        "        action = self.actor.forward(state)\n",
        "        action = self.onehot_from_logits(action)\n",
        "\n",
        "        return action\n",
        "    \n",
        "    def onehot_from_logits(self, logits, eps=0.0):\n",
        "        # get best (according to current policy) actions in one-hot form\n",
        "        argmax_acs = (logits == logits.max(0, keepdim=True)[0]).float()\n",
        "        if eps == 0.0:\n",
        "            return argmax_acs\n",
        "        # get random actions in one-hot form\n",
        "        rand_acs = Variable(torch.eye(logits.shape[1])[[np.random.choice(\n",
        "            range(logits.shape[1]), size=logits.shape[0])]], requires_grad=False)\n",
        "        # chooses between best and random actions using epsilon greedy\n",
        "        return torch.stack([argmax_acs[i] if r > eps else rand_acs[i] for i, r in\n",
        "                            enumerate(torch.rand(logits.shape[0]))])\n",
        "    \n",
        "    def update(self, indiv_reward_batch, indiv_obs_batch, global_state_batch, global_actions_batch, global_next_state_batch, next_global_actions):\n",
        "        \"\"\"\n",
        "        indiv_reward_batch      : only rewards of agent i\n",
        "        indiv_obs_batch         : only observations of agent i\n",
        "        global_state_batch      : observations of all agents are concatenated\n",
        "        global actions_batch    : actions of all agents are concatenated\n",
        "        global_next_state_batch : observations of all agents are concatenated\n",
        "        next_global_actions     : actions of all agents are concatenated\n",
        "        \"\"\"\n",
        "        indiv_reward_batch = torch.FloatTensor(indiv_reward_batch).to(self.device)\n",
        "        indiv_reward_batch = indiv_reward_batch.view(indiv_reward_batch.size(0), 1).to(self.device) \n",
        "        indiv_obs_batch = torch.FloatTensor(indiv_obs_batch).to(self.device)          \n",
        "        global_state_batch = torch.FloatTensor(global_state_batch).to(self.device)    \n",
        "        global_actions_batch = torch.stack(global_actions_batch).to(self.device)      \n",
        "        global_next_state_batch = torch.FloatTensor(global_next_state_batch).to(self.device)\n",
        "        next_global_actions = next_global_actions\n",
        "\n",
        "        # update critic        \n",
        "        self.critic_optimizer.zero_grad()\n",
        "        \n",
        "        curr_Q = self.critic.forward(global_state_batch, global_actions_batch)\n",
        "        next_Q = self.critic_target.forward(global_next_state_batch, next_global_actions)\n",
        "        estimated_Q = indiv_reward_batch + self.gamma * next_Q\n",
        "        \n",
        "        critic_loss = self.MSELoss(curr_Q, estimated_Q.detach())\n",
        "        critic_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # update actor\n",
        "        self.actor_optimizer.zero_grad()\n",
        "\n",
        "        policy_loss = -self.critic.forward(global_state_batch, global_actions_batch).mean()\n",
        "        curr_pol_out = self.actor.forward(indiv_obs_batch)\n",
        "        policy_loss += -(curr_pol_out**2).mean() * 1e-3 \n",
        "        policy_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
        "        self.actor_optimizer.step()\n",
        "    \n",
        "    def target_update(self):\n",
        "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "\n",
        "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
        "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHM3qRAkpXX8",
        "colab_type": "text"
      },
      "source": [
        "# **Replay Buffer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhHcRL6iZJLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OBXZd1mp81X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiAgentReplayBuffer:\n",
        "    \n",
        "    def __init__(self, num_agents, max_size):\n",
        "        self.max_size = max_size\n",
        "        self.num_agents = num_agents\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        experience = (state, action, np.array(reward), next_state, done)\n",
        "        self.buffer.append(experience)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        obs_batch = [[] for _ in range(self.num_agents)]  # [ [states of agent 1], ... ,[states of agent n] ]    ]\n",
        "        indiv_action_batch = [[] for _ in range(self.num_agents)] # [ [actions of agent 1], ... , [actions of agent n]]\n",
        "        indiv_reward_batch = [[] for _ in range(self.num_agents)]\n",
        "        next_obs_batch = [[] for _ in range(self.num_agents)]\n",
        "\n",
        "        global_state_batch = []\n",
        "        global_next_state_batch = []\n",
        "        global_actions_batch = []\n",
        "        done_batch = []\n",
        "\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "\n",
        "\n",
        "        for experience in batch:\n",
        "            state, action, reward, next_state, done = experience\n",
        "            \n",
        "            for i in range(self.num_agents):\n",
        "                obs_i = state[i]\n",
        "                action_i = action[i]\n",
        "                reward_i = reward[i]\n",
        "                next_obs_i = next_state[i]\n",
        "            \n",
        "                obs_batch[i].append(obs_i)\n",
        "                indiv_action_batch[i].append(action_i)\n",
        "                indiv_reward_batch[i].append(reward_i)\n",
        "                next_obs_batch[i].append(next_obs_i)\n",
        "\n",
        "            global_state_batch.append(np.concatenate(state))\n",
        "            global_actions_batch.append(torch.cat(action))\n",
        "            global_next_state_batch.append(np.concatenate(next_state))\n",
        "            done_batch.append(done)\n",
        "        \n",
        "        return obs_batch, indiv_action_batch, indiv_reward_batch, next_obs_batch, global_state_batch, global_actions_batch, global_next_state_batch, done_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTX2jTaHwylA",
        "colab_type": "text"
      },
      "source": [
        "# **Multi Agent Deep Deterministic Policy Gradient -  MADDPG**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBv70mN4hcVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTP_FNa8wtxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MADDPG:\n",
        "\n",
        "    def __init__(self, env, buffer_maxlen):\n",
        "        self.env = env\n",
        "        self.num_agents = env.n\n",
        "        self.replay_buffer = MultiAgentReplayBuffer(self.num_agents, buffer_maxlen)\n",
        "        self.agents = [DDPGAgent(self.env, i) for i in range(self.num_agents)]\n",
        "\n",
        "    def get_actions(self, states):\n",
        "        actions = []\n",
        "        for i in range(self.num_agents):\n",
        "            action = self.agents[i].get_action(states[i])\n",
        "            actions.append(action)\n",
        "        return actions\n",
        "\n",
        "    def update(self, batch_size):\n",
        "        obs_batch, indiv_action_batch, indiv_reward_batch, next_obs_batch, \\\n",
        "            global_state_batch, global_actions_batch, global_next_state_batch, done_batch = self.replay_buffer.sample(batch_size)\n",
        "        \n",
        "        for i in range(self.num_agents):\n",
        "            obs_batch_i = obs_batch[i]\n",
        "            indiv_action_batch_i = indiv_action_batch[i]\n",
        "            indiv_reward_batch_i = indiv_reward_batch[i]\n",
        "            next_obs_batch_i = next_obs_batch[i]\n",
        "\n",
        "            next_global_actions = []\n",
        "            for agent in self.agents:\n",
        "                next_obs_batch_i = torch.FloatTensor(next_obs_batch_i)\n",
        "                indiv_next_action = agent.actor.forward(next_obs_batch_i)\n",
        "                indiv_next_action = [agent.onehot_from_logits(indiv_next_action_j) for indiv_next_action_j in indiv_next_action]\n",
        "                indiv_next_action = torch.stack(indiv_next_action)\n",
        "                next_global_actions.append(indiv_next_action)\n",
        "            next_global_actions = torch.cat([next_actions_i for next_actions_i in next_global_actions], 1)\n",
        "\n",
        "            self.agents[i].update(indiv_reward_batch_i, obs_batch_i, global_state_batch, global_actions_batch, global_next_state_batch, next_global_actions)\n",
        "            self.agents[i].target_update()\n",
        "    \n",
        "    def run(self, max_episode, max_steps, batch_size):\n",
        "        episode_rewards = []\n",
        "        for episode in range(max_episode):\n",
        "            states = self.env.reset()\n",
        "            episode_reward = 0\n",
        "            for step in range(max_steps):\n",
        "                actions = self.get_actions(states)\n",
        "                next_states, rewards, dones, _ = self.env.step(actions)\n",
        "                episode_reward += np.mean(rewards)\n",
        "        \n",
        "                if all(dones) or step == max_steps - 1:\n",
        "                    dones = [1 for _ in range(self.num_agents)]\n",
        "                    self.replay_buffer.push(states, actions, rewards, next_states, dones)\n",
        "                    episode_rewards.append(episode_reward)\n",
        "                    print(\"episode: {}  |  reward: {}  \\n\".format(episode, np.round(episode_reward, decimals=4)))\n",
        "                    break\n",
        "                else:\n",
        "                    dones = [0 for _ in range(self.num_agents)]\n",
        "                    self.replay_buffer.push(states, actions, rewards, next_states, dones)\n",
        "                    states = next_states \n",
        "\n",
        "                    if len(self.replay_buffer) > batch_size:\n",
        "                        self.update(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tOzf7GTpgXR",
        "colab_type": "text"
      },
      "source": [
        "# **Spread Environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vTRfKG0pbFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from multiagent.environment import MultiAgentEnv\n",
        "import multiagent.scenarios as scenarios\n",
        "import torch \n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPAWXbVwpx1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_env(scenario_name, benchmark=False):\n",
        "    # load scenario from script\n",
        "    scenario = scenarios.load(scenario_name + \".py\").Scenario()\n",
        "    # create world\n",
        "    world = scenario.make_world()\n",
        "    # create multiagent environment\n",
        "    if benchmark:\n",
        "        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation, scenario.benchmark_data)\n",
        "    else:\n",
        "        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation)\n",
        "    return env\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uz_xasMqTz0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "71d34336-26b6-41a4-cbb4-80025d4c70ff"
      },
      "source": [
        "env = make_env(scenario_name=\"simple_spread\")\n",
        "\n",
        "ma_controller = MADDPG(env,1000000)\n",
        "ma_controller.run(500,300,32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0  |  reward: -21595.2177  \n",
            "\n",
            "episode: 1  |  reward: -18577.4276  \n",
            "\n",
            "episode: 2  |  reward: -3423.1299  \n",
            "\n",
            "episode: 3  |  reward: -7592.8313  \n",
            "\n",
            "episode: 4  |  reward: -3700.3921  \n",
            "\n",
            "episode: 5  |  reward: -5478.0207  \n",
            "\n",
            "episode: 6  |  reward: -4667.8869  \n",
            "\n",
            "episode: 7  |  reward: -8766.3405  \n",
            "\n",
            "episode: 8  |  reward: -4248.9941  \n",
            "\n",
            "episode: 9  |  reward: -5295.7437  \n",
            "\n",
            "episode: 10  |  reward: -2591.3975  \n",
            "\n",
            "episode: 11  |  reward: -3178.2135  \n",
            "\n",
            "episode: 12  |  reward: -6445.7617  \n",
            "\n",
            "episode: 13  |  reward: -5258.8974  \n",
            "\n",
            "episode: 14  |  reward: -5533.5524  \n",
            "\n",
            "episode: 15  |  reward: -5308.5812  \n",
            "\n",
            "episode: 16  |  reward: -5588.9711  \n",
            "\n",
            "episode: 17  |  reward: -3295.8217  \n",
            "\n",
            "episode: 18  |  reward: -4077.646  \n",
            "\n",
            "episode: 19  |  reward: -2478.0453  \n",
            "\n",
            "episode: 20  |  reward: -3928.5667  \n",
            "\n",
            "episode: 21  |  reward: -5452.3531  \n",
            "\n",
            "episode: 22  |  reward: -3197.169  \n",
            "\n",
            "episode: 23  |  reward: -4466.73  \n",
            "\n",
            "episode: 24  |  reward: -3353.5174  \n",
            "\n",
            "episode: 25  |  reward: -5352.8482  \n",
            "\n",
            "episode: 26  |  reward: -5970.293  \n",
            "\n",
            "episode: 27  |  reward: -5062.7746  \n",
            "\n",
            "episode: 28  |  reward: -4998.8274  \n",
            "\n",
            "episode: 29  |  reward: -2770.4328  \n",
            "\n",
            "episode: 30  |  reward: -5661.2486  \n",
            "\n",
            "episode: 31  |  reward: -4489.8825  \n",
            "\n",
            "episode: 32  |  reward: -3463.9256  \n",
            "\n",
            "episode: 33  |  reward: -3359.8841  \n",
            "\n",
            "episode: 34  |  reward: -3278.6753  \n",
            "\n",
            "episode: 35  |  reward: -2503.9081  \n",
            "\n",
            "episode: 36  |  reward: -2209.647  \n",
            "\n",
            "episode: 37  |  reward: -4034.2248  \n",
            "\n",
            "episode: 38  |  reward: -4212.6514  \n",
            "\n",
            "episode: 39  |  reward: -4501.6703  \n",
            "\n",
            "episode: 40  |  reward: -2741.8793  \n",
            "\n",
            "episode: 41  |  reward: -4017.6932  \n",
            "\n",
            "episode: 42  |  reward: -3497.2765  \n",
            "\n",
            "episode: 43  |  reward: -3361.4472  \n",
            "\n",
            "episode: 44  |  reward: -2971.7349  \n",
            "\n",
            "episode: 45  |  reward: -3829.4859  \n",
            "\n",
            "episode: 46  |  reward: -3049.2078  \n",
            "\n",
            "episode: 47  |  reward: -3566.3479  \n",
            "\n",
            "episode: 48  |  reward: -3996.9127  \n",
            "\n",
            "episode: 49  |  reward: -3759.8478  \n",
            "\n",
            "episode: 50  |  reward: -4686.751  \n",
            "\n",
            "episode: 51  |  reward: -3820.055  \n",
            "\n",
            "episode: 52  |  reward: -4037.0375  \n",
            "\n",
            "episode: 53  |  reward: -4559.674  \n",
            "\n",
            "episode: 54  |  reward: -3606.2632  \n",
            "\n",
            "episode: 55  |  reward: -3634.2579  \n",
            "\n",
            "episode: 56  |  reward: -3191.6082  \n",
            "\n",
            "episode: 57  |  reward: -5996.282  \n",
            "\n",
            "episode: 58  |  reward: -3688.2651  \n",
            "\n",
            "episode: 59  |  reward: -5336.1507  \n",
            "\n",
            "episode: 60  |  reward: -4105.5616  \n",
            "\n",
            "episode: 61  |  reward: -3571.551  \n",
            "\n",
            "episode: 62  |  reward: -4518.8906  \n",
            "\n",
            "episode: 63  |  reward: -3296.2814  \n",
            "\n",
            "episode: 64  |  reward: -2902.7993  \n",
            "\n",
            "episode: 65  |  reward: -4022.5026  \n",
            "\n",
            "episode: 66  |  reward: -2064.4225  \n",
            "\n",
            "episode: 67  |  reward: -3592.0008  \n",
            "\n",
            "episode: 68  |  reward: -3207.2426  \n",
            "\n",
            "episode: 69  |  reward: -3054.9307  \n",
            "\n",
            "episode: 70  |  reward: -4122.5063  \n",
            "\n",
            "episode: 71  |  reward: -3242.7009  \n",
            "\n",
            "episode: 72  |  reward: -4959.2111  \n",
            "\n",
            "episode: 73  |  reward: -4273.8022  \n",
            "\n",
            "episode: 74  |  reward: -2975.8285  \n",
            "\n",
            "episode: 75  |  reward: -3645.8621  \n",
            "\n",
            "episode: 76  |  reward: -2732.2291  \n",
            "\n",
            "episode: 77  |  reward: -2997.3204  \n",
            "\n",
            "episode: 78  |  reward: -3696.6471  \n",
            "\n",
            "episode: 79  |  reward: -3027.9366  \n",
            "\n",
            "episode: 80  |  reward: -3640.4823  \n",
            "\n",
            "episode: 81  |  reward: -3319.0063  \n",
            "\n",
            "episode: 82  |  reward: -3447.9825  \n",
            "\n",
            "episode: 83  |  reward: -3565.8707  \n",
            "\n",
            "episode: 84  |  reward: -4968.6929  \n",
            "\n",
            "episode: 85  |  reward: -5861.8082  \n",
            "\n",
            "episode: 86  |  reward: -3062.6313  \n",
            "\n",
            "episode: 87  |  reward: -2769.5226  \n",
            "\n",
            "episode: 88  |  reward: -3778.2023  \n",
            "\n",
            "episode: 89  |  reward: -3632.4894  \n",
            "\n",
            "episode: 90  |  reward: -2368.666  \n",
            "\n",
            "episode: 91  |  reward: -2587.3475  \n",
            "\n",
            "episode: 92  |  reward: -3578.9782  \n",
            "\n",
            "episode: 93  |  reward: -5114.954  \n",
            "\n",
            "episode: 94  |  reward: -3642.6573  \n",
            "\n",
            "episode: 95  |  reward: -3294.5007  \n",
            "\n",
            "episode: 96  |  reward: -5139.4991  \n",
            "\n",
            "episode: 97  |  reward: -2891.2204  \n",
            "\n",
            "episode: 98  |  reward: -4010.2002  \n",
            "\n",
            "episode: 99  |  reward: -3973.4433  \n",
            "\n",
            "episode: 100  |  reward: -2612.7998  \n",
            "\n",
            "episode: 101  |  reward: -4043.4438  \n",
            "\n",
            "episode: 102  |  reward: -2952.1691  \n",
            "\n",
            "episode: 103  |  reward: -4302.301  \n",
            "\n",
            "episode: 104  |  reward: -4503.4642  \n",
            "\n",
            "episode: 105  |  reward: -3183.7391  \n",
            "\n",
            "episode: 106  |  reward: -4212.8645  \n",
            "\n",
            "episode: 107  |  reward: -3382.6935  \n",
            "\n",
            "episode: 108  |  reward: -3565.3249  \n",
            "\n",
            "episode: 109  |  reward: -3411.9466  \n",
            "\n",
            "episode: 110  |  reward: -3242.8092  \n",
            "\n",
            "episode: 111  |  reward: -2857.3273  \n",
            "\n",
            "episode: 112  |  reward: -3404.6902  \n",
            "\n",
            "episode: 113  |  reward: -4984.6166  \n",
            "\n",
            "episode: 114  |  reward: -3990.933  \n",
            "\n",
            "episode: 115  |  reward: -3675.499  \n",
            "\n",
            "episode: 116  |  reward: -3013.9285  \n",
            "\n",
            "episode: 117  |  reward: -3324.2933  \n",
            "\n",
            "episode: 118  |  reward: -4080.4056  \n",
            "\n",
            "episode: 119  |  reward: -2846.6642  \n",
            "\n",
            "episode: 120  |  reward: -5001.5984  \n",
            "\n",
            "episode: 121  |  reward: -3590.5281  \n",
            "\n",
            "episode: 122  |  reward: -3178.4539  \n",
            "\n",
            "episode: 123  |  reward: -2903.6765  \n",
            "\n",
            "episode: 124  |  reward: -3948.973  \n",
            "\n",
            "episode: 125  |  reward: -5146.9828  \n",
            "\n",
            "episode: 126  |  reward: -4006.9438  \n",
            "\n",
            "episode: 127  |  reward: -2916.4097  \n",
            "\n",
            "episode: 128  |  reward: -3790.6441  \n",
            "\n",
            "episode: 129  |  reward: -3410.6403  \n",
            "\n",
            "episode: 130  |  reward: -4621.4794  \n",
            "\n",
            "episode: 131  |  reward: -3287.1433  \n",
            "\n",
            "episode: 132  |  reward: -4228.4461  \n",
            "\n",
            "episode: 133  |  reward: -5176.0639  \n",
            "\n",
            "episode: 134  |  reward: -3306.7707  \n",
            "\n",
            "episode: 135  |  reward: -5275.8018  \n",
            "\n",
            "episode: 136  |  reward: -4391.6309  \n",
            "\n",
            "episode: 137  |  reward: -2782.2839  \n",
            "\n",
            "episode: 138  |  reward: -4148.6145  \n",
            "\n",
            "episode: 139  |  reward: -3393.6169  \n",
            "\n",
            "episode: 140  |  reward: -3673.6084  \n",
            "\n",
            "episode: 141  |  reward: -3643.016  \n",
            "\n",
            "episode: 142  |  reward: -4090.8497  \n",
            "\n",
            "episode: 143  |  reward: -2550.0019  \n",
            "\n",
            "episode: 144  |  reward: -5187.5934  \n",
            "\n",
            "episode: 145  |  reward: -2577.9635  \n",
            "\n",
            "episode: 146  |  reward: -3350.8616  \n",
            "\n",
            "episode: 147  |  reward: -4456.8995  \n",
            "\n",
            "episode: 148  |  reward: -3445.6034  \n",
            "\n",
            "episode: 149  |  reward: -4563.2224  \n",
            "\n",
            "episode: 150  |  reward: -3419.1869  \n",
            "\n",
            "episode: 151  |  reward: -4232.8399  \n",
            "\n",
            "episode: 152  |  reward: -3410.2397  \n",
            "\n",
            "episode: 153  |  reward: -4232.6536  \n",
            "\n",
            "episode: 154  |  reward: -2197.2289  \n",
            "\n",
            "episode: 155  |  reward: -3736.9798  \n",
            "\n",
            "episode: 156  |  reward: -3974.9026  \n",
            "\n",
            "episode: 157  |  reward: -2278.8347  \n",
            "\n",
            "episode: 158  |  reward: -3564.8227  \n",
            "\n",
            "episode: 159  |  reward: -3926.0035  \n",
            "\n",
            "episode: 160  |  reward: -4286.7521  \n",
            "\n",
            "episode: 161  |  reward: -2940.891  \n",
            "\n",
            "episode: 162  |  reward: -2142.7587  \n",
            "\n",
            "episode: 163  |  reward: -3818.4863  \n",
            "\n",
            "episode: 164  |  reward: -3586.5244  \n",
            "\n",
            "episode: 165  |  reward: -5562.9514  \n",
            "\n",
            "episode: 166  |  reward: -4569.2046  \n",
            "\n",
            "episode: 167  |  reward: -3690.6774  \n",
            "\n",
            "episode: 168  |  reward: -3663.2013  \n",
            "\n",
            "episode: 169  |  reward: -2998.0656  \n",
            "\n",
            "episode: 170  |  reward: -3466.4983  \n",
            "\n",
            "episode: 171  |  reward: -3935.8436  \n",
            "\n",
            "episode: 172  |  reward: -3385.4056  \n",
            "\n",
            "episode: 173  |  reward: -3561.5782  \n",
            "\n",
            "episode: 174  |  reward: -3774.6551  \n",
            "\n",
            "episode: 175  |  reward: -2338.7534  \n",
            "\n",
            "episode: 176  |  reward: -5843.9914  \n",
            "\n",
            "episode: 177  |  reward: -3705.8617  \n",
            "\n",
            "episode: 178  |  reward: -4668.3857  \n",
            "\n",
            "episode: 179  |  reward: -3043.2623  \n",
            "\n",
            "episode: 180  |  reward: -4589.9176  \n",
            "\n",
            "episode: 181  |  reward: -3851.5105  \n",
            "\n",
            "episode: 182  |  reward: -3891.0229  \n",
            "\n",
            "episode: 183  |  reward: -2321.5402  \n",
            "\n",
            "episode: 184  |  reward: -3958.3301  \n",
            "\n",
            "episode: 185  |  reward: -4011.6835  \n",
            "\n",
            "episode: 186  |  reward: -3034.7342  \n",
            "\n",
            "episode: 187  |  reward: -3666.7174  \n",
            "\n",
            "episode: 188  |  reward: -1750.3643  \n",
            "\n",
            "episode: 189  |  reward: -4133.4321  \n",
            "\n",
            "episode: 190  |  reward: -3457.2759  \n",
            "\n",
            "episode: 191  |  reward: -5154.0886  \n",
            "\n",
            "episode: 192  |  reward: -5319.6465  \n",
            "\n",
            "episode: 193  |  reward: -4244.173  \n",
            "\n",
            "episode: 194  |  reward: -4310.9898  \n",
            "\n",
            "episode: 195  |  reward: -2075.0659  \n",
            "\n",
            "episode: 196  |  reward: -4594.3531  \n",
            "\n",
            "episode: 197  |  reward: -3814.7085  \n",
            "\n",
            "episode: 198  |  reward: -2150.2525  \n",
            "\n",
            "episode: 199  |  reward: -4241.6173  \n",
            "\n",
            "episode: 200  |  reward: -3367.7919  \n",
            "\n",
            "episode: 201  |  reward: -3191.0119  \n",
            "\n",
            "episode: 202  |  reward: -3539.9282  \n",
            "\n",
            "episode: 203  |  reward: -4113.5251  \n",
            "\n",
            "episode: 204  |  reward: -4415.116  \n",
            "\n",
            "episode: 205  |  reward: -5206.8421  \n",
            "\n",
            "episode: 206  |  reward: -5100.1473  \n",
            "\n",
            "episode: 207  |  reward: -4400.6741  \n",
            "\n",
            "episode: 208  |  reward: -5003.5379  \n",
            "\n",
            "episode: 209  |  reward: -3050.7372  \n",
            "\n",
            "episode: 210  |  reward: -2530.5482  \n",
            "\n",
            "episode: 211  |  reward: -2894.5998  \n",
            "\n",
            "episode: 212  |  reward: -4096.4575  \n",
            "\n",
            "episode: 213  |  reward: -4131.5259  \n",
            "\n",
            "episode: 214  |  reward: -1897.4339  \n",
            "\n",
            "episode: 215  |  reward: -3464.1115  \n",
            "\n",
            "episode: 216  |  reward: -2966.5675  \n",
            "\n",
            "episode: 217  |  reward: -3905.4976  \n",
            "\n",
            "episode: 218  |  reward: -4039.1143  \n",
            "\n",
            "episode: 219  |  reward: -3657.3148  \n",
            "\n",
            "episode: 220  |  reward: -3498.0336  \n",
            "\n",
            "episode: 221  |  reward: -4062.8277  \n",
            "\n",
            "episode: 222  |  reward: -4492.8182  \n",
            "\n",
            "episode: 223  |  reward: -4345.7518  \n",
            "\n",
            "episode: 224  |  reward: -4248.0416  \n",
            "\n",
            "episode: 225  |  reward: -4118.7164  \n",
            "\n",
            "episode: 226  |  reward: -3764.0122  \n",
            "\n",
            "episode: 227  |  reward: -3788.8117  \n",
            "\n",
            "episode: 228  |  reward: -3199.0663  \n",
            "\n",
            "episode: 229  |  reward: -4023.8073  \n",
            "\n",
            "episode: 230  |  reward: -3790.6113  \n",
            "\n",
            "episode: 231  |  reward: -3906.6116  \n",
            "\n",
            "episode: 232  |  reward: -4925.1347  \n",
            "\n",
            "episode: 233  |  reward: -3878.7371  \n",
            "\n",
            "episode: 234  |  reward: -3510.2047  \n",
            "\n",
            "episode: 235  |  reward: -3770.3515  \n",
            "\n",
            "episode: 236  |  reward: -3747.1483  \n",
            "\n",
            "episode: 237  |  reward: -3984.4411  \n",
            "\n",
            "episode: 238  |  reward: -4648.9113  \n",
            "\n",
            "episode: 239  |  reward: -4416.7628  \n",
            "\n",
            "episode: 240  |  reward: -3485.7103  \n",
            "\n",
            "episode: 241  |  reward: -2998.0789  \n",
            "\n",
            "episode: 242  |  reward: -5966.0062  \n",
            "\n",
            "episode: 243  |  reward: -5751.3449  \n",
            "\n",
            "episode: 244  |  reward: -3839.115  \n",
            "\n",
            "episode: 245  |  reward: -2955.8735  \n",
            "\n",
            "episode: 246  |  reward: -3900.5335  \n",
            "\n",
            "episode: 247  |  reward: -1899.0452  \n",
            "\n",
            "episode: 248  |  reward: -3995.8016  \n",
            "\n",
            "episode: 249  |  reward: -4669.7982  \n",
            "\n",
            "episode: 250  |  reward: -3601.9604  \n",
            "\n",
            "episode: 251  |  reward: -6514.0746  \n",
            "\n",
            "episode: 252  |  reward: -4899.3019  \n",
            "\n",
            "episode: 253  |  reward: -3897.7063  \n",
            "\n",
            "episode: 254  |  reward: -3271.808  \n",
            "\n",
            "episode: 255  |  reward: -5150.8935  \n",
            "\n",
            "episode: 256  |  reward: -2939.9475  \n",
            "\n",
            "episode: 257  |  reward: -1900.5489  \n",
            "\n",
            "episode: 258  |  reward: -2957.5845  \n",
            "\n",
            "episode: 259  |  reward: -3177.2759  \n",
            "\n",
            "episode: 260  |  reward: -6186.831  \n",
            "\n",
            "episode: 261  |  reward: -4307.5815  \n",
            "\n",
            "episode: 262  |  reward: -3733.9881  \n",
            "\n",
            "episode: 263  |  reward: -3214.0878  \n",
            "\n",
            "episode: 264  |  reward: -3545.7245  \n",
            "\n",
            "episode: 265  |  reward: -3522.0272  \n",
            "\n",
            "episode: 266  |  reward: -4756.1698  \n",
            "\n",
            "episode: 267  |  reward: -5020.908  \n",
            "\n",
            "episode: 268  |  reward: -4807.9462  \n",
            "\n",
            "episode: 269  |  reward: -3032.9701  \n",
            "\n",
            "episode: 270  |  reward: -4985.5759  \n",
            "\n",
            "episode: 271  |  reward: -3760.0517  \n",
            "\n",
            "episode: 272  |  reward: -3257.7732  \n",
            "\n",
            "episode: 273  |  reward: -3676.6676  \n",
            "\n",
            "episode: 274  |  reward: -3127.4387  \n",
            "\n",
            "episode: 275  |  reward: -3239.0749  \n",
            "\n",
            "episode: 276  |  reward: -3639.6348  \n",
            "\n",
            "episode: 277  |  reward: -3028.1346  \n",
            "\n",
            "episode: 278  |  reward: -2737.4388  \n",
            "\n",
            "episode: 279  |  reward: -2833.3314  \n",
            "\n",
            "episode: 280  |  reward: -2919.7303  \n",
            "\n",
            "episode: 281  |  reward: -3703.1752  \n",
            "\n",
            "episode: 282  |  reward: -3552.2747  \n",
            "\n",
            "episode: 283  |  reward: -2882.4055  \n",
            "\n",
            "episode: 284  |  reward: -3261.2189  \n",
            "\n",
            "episode: 285  |  reward: -4453.3708  \n",
            "\n",
            "episode: 286  |  reward: -4068.2851  \n",
            "\n",
            "episode: 287  |  reward: -5256.969  \n",
            "\n",
            "episode: 288  |  reward: -2343.0148  \n",
            "\n",
            "episode: 289  |  reward: -2912.6385  \n",
            "\n",
            "episode: 290  |  reward: -6081.0355  \n",
            "\n",
            "episode: 291  |  reward: -2761.2646  \n",
            "\n",
            "episode: 292  |  reward: -4243.5366  \n",
            "\n",
            "episode: 293  |  reward: -5544.9524  \n",
            "\n",
            "episode: 294  |  reward: -3764.634  \n",
            "\n",
            "episode: 295  |  reward: -4329.286  \n",
            "\n",
            "episode: 296  |  reward: -3972.0441  \n",
            "\n",
            "episode: 297  |  reward: -6091.0179  \n",
            "\n",
            "episode: 298  |  reward: -3635.8972  \n",
            "\n",
            "episode: 299  |  reward: -3386.1423  \n",
            "\n",
            "episode: 300  |  reward: -2979.8476  \n",
            "\n",
            "episode: 301  |  reward: -2607.6988  \n",
            "\n",
            "episode: 302  |  reward: -5255.3351  \n",
            "\n",
            "episode: 303  |  reward: -2796.6436  \n",
            "\n",
            "episode: 304  |  reward: -3548.3756  \n",
            "\n",
            "episode: 305  |  reward: -2956.6121  \n",
            "\n",
            "episode: 306  |  reward: -3154.4956  \n",
            "\n",
            "episode: 307  |  reward: -4729.1259  \n",
            "\n",
            "episode: 308  |  reward: -2540.2528  \n",
            "\n",
            "episode: 309  |  reward: -3683.328  \n",
            "\n",
            "episode: 310  |  reward: -5441.1688  \n",
            "\n",
            "episode: 311  |  reward: -5222.357  \n",
            "\n",
            "episode: 312  |  reward: -2670.3238  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GQ1llHTqe0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}